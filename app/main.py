import asyncio
import datetime
from typing import List, Dict

from config import config
from logging_config import logger
from scheduler.task_scheduler import task_scheduler
from services.news_crawler_service import crawler_service
from nlp.processor import EnhancedNewsProcessor
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker


class NewsMonitorApp:
    def __init__(self):
        # åˆå§‹åŒ–æ•°æ®åº“
        self.engine = create_engine(config.DATABASE_URL)
        Base.metadata.create_all(self.engine)
        self.Session = sessionmaker(bind=self.engine)

        # åˆå§‹åŒ–å¤„ç†å™¨
        self.processor = EnhancedNewsProcessor()

        # æ³¨å†Œå®šæ—¶ä»»åŠ¡
        self._register_tasks()

    def _register_tasks(self):
        """æ³¨å†Œæ‰€æœ‰å®šæ—¶ä»»åŠ¡"""
        # ä¸»è¦æŠ“å–ä»»åŠ¡ - æ¯30åˆ†é’Ÿ
        task_scheduler.add_interval_job(
            self.main_crawl_task,
            config.CRAWL_INTERVAL_MINUTES,
            'main_crawl_task'
        )

        # å¿«é€Ÿæ£€æŸ¥ä»»åŠ¡ - æ¯5åˆ†é’Ÿï¼ˆåªåœ¨å·¥ä½œæ—¶é—´ï¼‰
        task_scheduler.add_daily_time_range_job(
            self.quick_check_task,
            config.WORKING_HOURS['start'],
            config.WORKING_HOURS['end'],
            config.QUICK_CHECK_INTERVAL_MINUTES,
            'quick_check_task'
        )

        # æ¯æ—¥æ¸…ç†ä»»åŠ¡ - å‡Œæ™¨2ç‚¹
        task_scheduler.add_cron_job(
            self.daily_cleanup_task,
            '0 2 * * *',
            'daily_cleanup_task'
        )

        # çŠ¶æ€æŠ¥å‘Šä»»åŠ¡ - æ¯å°æ—¶
        task_scheduler.add_interval_job(
            self.status_report_task,
            60,
            'status_report_task'
        )

    async def main_crawl_task(self):
        """ä¸»è¦æŠ“å–ä»»åŠ¡"""
        logger.info("å¼€å§‹æ‰§è¡Œä¸»è¦æŠ“å–ä»»åŠ¡...")

        try:
            # æŠ“å–æ–°é—»
            news_items = await crawler_service.crawl_all_sources()
            logger.info(f"æŠ“å–åˆ° {len(news_items)} æ¡æ–°é—»")

            # å¤„ç†æ–°é—»
            processed_count = 0
            black_swan_count = 0

            for item in news_items:
                try:
                    processed = await self.processor.process_news_async(item)
                    await self._save_news_item(processed)

                    processed_count += 1
                    if processed['is_black_swan']:
                        black_swan_count += 1
                        await self._alert_black_swan(processed)

                except Exception as e:
                    logger.error(f"å¤„ç†æ–°é—»å¤±è´¥: {e}")

            logger.info(f"å¤„ç†å®Œæˆ: {processed_count} æ¡æ–°é—», {black_swan_count} æ¡é»‘å¤©é¹…äº‹ä»¶")

        except Exception as e:
            logger.error(f"ä¸»è¦æŠ“å–ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")

    async def quick_check_task(self):
        """å¿«é€Ÿæ£€æŸ¥ä»»åŠ¡"""
        logger.debug("æ‰§è¡Œå¿«é€Ÿæ£€æŸ¥ä»»åŠ¡...")
        # å¯ä»¥å®ç°å¢é‡æ£€æŸ¥æˆ–è€…é‡ç‚¹æºæ£€æŸ¥
        # è¿™é‡Œå¯ä»¥åªæ£€æŸ¥é«˜ä¼˜å…ˆçº§çš„æ–°é—»æº

    async def daily_cleanup_task(self):
        """æ¯æ—¥æ¸…ç†ä»»åŠ¡"""
        logger.info("æ‰§è¡Œæ¯æ—¥æ¸…ç†ä»»åŠ¡...")
        # æ¸…ç†æ—§æ•°æ®ã€ä¼˜åŒ–æ•°æ®åº“ç­‰

    async def status_report_task(self):
        """çŠ¶æ€æŠ¥å‘Šä»»åŠ¡"""
        # ç”Ÿæˆç³»ç»ŸçŠ¶æ€æŠ¥å‘Š
        session = self.Session()
        try:
            total_news = session.query(NewsArticle).count()
            black_swan_news = session.query(NewsArticle).filter_by(is_black_swan=1).count()

            logger.info(f"ç³»ç»ŸçŠ¶æ€ - æ€»æ–°é—»: {total_news}, é»‘å¤©é¹…äº‹ä»¶: {black_swan_news}")

        except Exception as e:
            logger.error(f"çŠ¶æ€æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        finally:
            session.close()

    async def _save_news_item(self, news_data: Dict):
        """ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“"""
        session = self.Session()
        try:
            article = NewsArticle(
                title=news_data['title'],
                content=news_data['content'][:5000],
                url=news_data['url'],
                source=news_data['source'],
                published_at=news_data.get('published_at'),
                sentiment_score=news_data['gpt_analysis'].get('sentiment', 0),
                black_swan_score=news_data['final_black_swan_score'],
                is_black_swan=1 if news_data['is_black_swan'] else 0,
                categories=','.join(news_data['gpt_analysis'].get('impact_areas', [])),
                risk_level=news_data['gpt_analysis'].get('risk_level', 'unknown')
            )

            session.add(article)
            session.commit()
            logger.debug(f"ä¿å­˜æ–°é—»: {news_data['title'][:50]}...")

        except Exception as e:
            session.rollback()
            logger.error(f"ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        finally:
            session.close()

    async def _alert_black_swan(self, news_data: Dict):
        """é»‘å¤©é¹…äº‹ä»¶è­¦æŠ¥"""
        logger.warning(f"ğŸš¨ é»‘å¤©é¹…äº‹ä»¶è­¦æŠ¥ ğŸš¨")
        logger.warning(f"æ ‡é¢˜: {news_data['title']}")
        logger.warning(f"è¯„åˆ†: {news_data['final_black_swan_score']:.3f}")
        logger.warning(f"é“¾æ¥: {news_data['url']}")

        # è¿™é‡Œå¯ä»¥é›†æˆå„ç§è­¦æŠ¥æ–¹å¼
        # await self._send_email_alert(news_data)
        # await self._send_slack_alert(news_data)

    def run(self):
        """è¿è¡Œåº”ç”¨"""
        logger.info("æ–°é—»ç›‘æ§åº”ç”¨å¯åŠ¨ä¸­...")
        logger.info(f"æ•°æ®åº“: {config.DATABASE_URL}")
        logger.info(f"æŠ“å–é—´éš”: {config.CRAWL_INTERVAL_MINUTES} åˆ†é’Ÿ")
        logger.info(f"æ–°é—»æºæ•°é‡: {len(config.NEWS_SOURCES)}")

        # å¯åŠ¨è°ƒåº¦å™¨
        task_scheduler.start()


# åº”ç”¨å¯åŠ¨
if __name__ == "__main__":
    # åˆ›å»ºäº‹ä»¶å¾ªç¯
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    try:
        app = NewsMonitorApp()
        app.run()
    except KeyboardInterrupt:
        logger.info("åº”ç”¨è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"åº”ç”¨å¯åŠ¨å¤±è´¥: {e}")
    finally:
        # æ¸…ç†èµ„æº
        loop.run_until_complete(crawler_service.close_session())
        loop.close()